<!doctype html>
<meta charset="UTF-8">
<html lang="en">
<head>
<title>Your Project Name</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Retention Networks Explained</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Retentive Network: A Successor to Transformer for Large Language Models</h2>
<p>The paper introduces the Retentive Network (RetNet), which is designed to address (claims) some of the limitations of the Transformer 
  architecture, particularly in the context of large language models. The key advantages of RetNet over 
  traditional Transformers are centered around its ability to achieve training parallelism, good performance, 
  and low inference cost simultaneously, which the authors refer to as the "impossible triangle."<br> 
  <br>
  The reason this paper interests me is because if the claims of the paper are true, this will be next big thing in language modelling
</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Literature Review</h2>

<p><strong>"Attention Is All You Need" (Vaswani et al., 2017)</strong></p>
<ul>
    <li>Introduced the Transformer model, focusing solely on attention mechanisms, eliminating the need for recurrence and convolutions.</li>
    <li>Achieved state-of-the-art results in machine translation, proving more parallelizable and efficient in training.</li>
</ul>

<p><strong>"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" (Katharopoulos et al., 2020)</strong></p>
<ul>
    <li>Proposed linear transformers to reduce complexity from O(N^2) to O(N), making them significantly faster for long sequences.</li>
    <li>Maintained performance comparable to standard transformers while being up to 4000x faster in autoregressive prediction.</li>
</ul>

<p><strong>"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" (Dao et al., 2022)</strong></p>
<ul>
    <li>Introduced FlashAttention, an IO-aware attention algorithm optimizing memory usage between GPU memory levels.</li>
    <li>Enabled faster Transformer training and improved performance on various tasks, including long-sequence challenges.</li>
</ul>

<p><strong>"Retentive Network: A Successor to Transformer for Large Language Models" (Sun et al., 2023) </strong></p>
<ul>
    <li>Proposed RETNET, a new architecture for large language models, achieving efficient training, low-cost inference, and strong performance.</li>
    <li>Introduced a retention mechanism supporting various computation paradigms, positioning it as a successor to the Transformer.</li>
</ul>

<!-- <h2>Biography</h2> -->
<h2>Biography</h2>

<table>
  <!-- Row for Names -->
  <tr>
    <td><p>Yutao Sun</p></td>
    <td><p>Li Dong</p></td>
    <td><p>Shaohan Huang</p></td>
    <td><p>Shuming Ma</p></td>
  </tr>

  <!-- Row for Images and Qualifications -->
  <tr>
    <!-- Person 1 -->
    <td>
      <img src="images/image1.jpg" alt="Yutao Sun" style="width:100px;height:100px;">
      <p>Research intern in Microsoft Research Lab - Asia<br>
      PhD student in Tsinghua University, Beijing<br>
      Research interest in LLM backbone & applications, and long sequence modeling and inference</p>
    </td>
    <!-- Person 2 -->
    <td>
      <img src="images/image2.jpg" alt="Li Dong" style="width:100px;height:100px;">
      <p>Principal Researcher, Microsoft Research Lab - Asia<br>
      PhD in Informatics from The University of Edinburgh, Scotland<br>
      Research interest in Human Language Technologies and Natural Language Computing</p>
    </td>
    <!-- Person 3 -->
    <td>
      <img src="images/image3.jpg" alt="Shaohan Huang" style="width:100px;height:100px;">
      <p>Senior Researcher at the Natural Language Computing group at Microsoft Research Lab - Asia<br>
      MS in Computer Science at Beihang University, Beijing<br>
      Recently worked on a paper called: Language is not all you need</p>
    </td>
    <!-- Person 4 -->
    <td>
      <img src="images/image4.jpg" alt="Shuming Ma" style="width:100px;height:100px;">
      <p>Researcher at the Natural Language Computing group at Microsoft Research Lab - Asia<br>
      Master’s and Bachelor’s degrees from Peking University, with a focus on natural language processing<br>
      Published 30+ papers on large-scale pre-trained LMs at top conferences (ICML, ICLR, ACL, EMNLP)</p>
    </td>
  </tr>

  <!-- Row for Names (Second Row of People) -->
  <tr>
    <td><p>Yuqing Xia</p></td>
    <td><p>Jilong Xue</p></td>
    <td><p>Furu Wei</p></td>
    <td><p>Jianyong Wang</p></td>
  </tr>

  <!-- Row for Images and Qualifications (Second Row of People) -->
  <tr>
    <!-- Person 5 -->
    <td>
      <img src="images/image5.png" alt="Yuqing Xia" style="width:100px;height:100px;">
      <p>Researcher at the Natural Language Computing group at Microsoft Research Lab - Asia<br>
      Ph.D. in Biology from Peking University in 2019<br>
      Interested in using AI to empower natural science research</p>
    </td>
    <!-- Person 6 -->
    <td>
      <img src="images/image6.jpg" alt="Jilong Xue" style="width:100px;height:100px;">
      <p>Principal Researcher in System Research Group of Microsoft Research Asia (MSRA)<br>
      Jilong received his Ph.D. in Computer Science from Peking University<br>
      Builds AI frameworks and compilers to bridge and optimize hardware for AI applications</p>
    </td>
    <!-- Person 7 -->
    <td>
      <img src="images/image7.jpg" alt="Furu Wei" style="width:100px;height:100px;">
      <p>Partner Research Manager at Microsoft Research Asia<br>
      Previously a Staff Researcher at IBM Research - China<br>
      Ph.D. in computer science from Wuhan University in 2009<br>
      Worked with Li Dong on several Microsoft research papers</p>
    </td>
    <!-- Person 8 -->
    <td>
      <img src="images/image8.png" alt="Jianyong Wang" style="width:100px;height:100px;">
      <p>Professor in the Department of Computer Science and Technology at Tsinghua University<br>
      Supported by various organizations<br>
      Research focuses on various data mining algorithms</p>
    </td>
  </tr>
</table>

<h2>Visualizations</h2>


<h2>Social Impact</h2>
<h2>Industry Applications</h2>
<h2>Follow-on Research</h2>
<h2>Peer-Review</h2>

<div>
    <h3>Review by Karan Mudaliar</h3>
    <p><strong>Score:</strong> NA</p>
    <p><strong>Strengths:</strong></p>
    <ul>
        <li>bullet points</li>
    </ul>
    <p><strong>Weaknesses:</strong></p>
    <ul>
        <li>Extremely long paper</li>
        <li>bullet points</li>
    </ul>
</div>

<div>
    <h3>Review by Sashank</h3>
    <p><strong>Score:</strong> NA</p>
    <p><strong>Strengths:</strong></p>
    <ul>
        <li>bullet points</li>
    </ul>
    <p><strong>Weaknesses:</strong></p>
    <ul>
        <li>bullet points</li>
    </ul>
</div>


<p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on your findings.
</p>

<h3>References</h3>

<p><a name="FlashAttention">[1]</a> <a href="https://arxiv.org/pdf/2205.14135.pdf"
  >Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
  <em>Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness</em></a>
</p>
<p><a name="Attention is all you need">[2]</a> <a href="https://arxiv.org/pdf/1706.03762.pdf"
  >Vaswani et al.
  <em>Attention Is All You Need</em></a>
</p>
<p><a name="Transformers are RNNs">[3]</a> <a href="https://arxiv.org/pdf/2006.16236.pdf"
  >Angelos Katharopoulos & Apoorv Vyas et al.
  <em>Transformers are RNNs:</em></a>
  Fast Autoregressive Transformers with Linear Attention.
</p>
<p><a name="Retentive Network">[4]</a> <a href="https://arxiv.org/pdf/2307.08621.pdf"
  >Yutao Sun & Li Dong et al.
  <em>Retentive Network:</em></a>
  A Successor to Transformer for Large Language Models
</p>

<!-- <h2>Team Members</h2>
                                                   
<p>Make sure to list here is who is on the team.</p> -->

<h2>Team Members</h2>

<div class="team-member">
  <img src="images/Karan.jpeg" alt="Karan" width="80" height="80">
  <p>Karan Mudaliar</p>
</div>

<div class="team-member">
  <img src="images/Sashank.jpeg" alt="Sashank" width="80" height="80">
  <p>Sashank Reddy</p>
</div>



  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
